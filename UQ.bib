@online{arbelPrimerBayesianNeural2023,
  title = {A {{Primer}} on {{Bayesian Neural Networks}}: {{Review}} and {{Debates}}},
  shorttitle = {A {{Primer}} on {{Bayesian Neural Networks}}},
  author = {Arbel, Julyan and Pitas, Konstantinos and Vladimirova, Mariia and Fortuin, Vincent},
  date = {2023-09-28},
  eprint = {2309.16314},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.2309.16314},
  url = {http://arxiv.org/abs/2309.16314},
  urldate = {2025-09-08},
  abstract = {Neural networks have achieved remarkable performance across various problem domains, but their widespread applicability is hindered by inherent limitations such as overconfidence in predictions, lack of interpretability, and vulnerability to adversarial attacks. To address these challenges, Bayesian neural networks (BNNs) have emerged as a compelling extension of conventional neural networks, integrating uncertainty estimation into their predictive capabilities. This comprehensive primer presents a systematic introduction to the fundamental concepts of neural networks and Bayesian inference, elucidating their synergistic integration for the development of BNNs. The target audience comprises statisticians with a potential background in Bayesian methods but lacking deep learning expertise, as well as machine learners proficient in deep neural networks but with limited exposure to Bayesian statistics. We provide an overview of commonly employed priors, examining their impact on model behavior and performance. Additionally, we delve into the practical considerations associated with training and inference in BNNs. Furthermore, we explore advanced topics within the realm of BNN research, acknowledging the existence of ongoing debates and controversies. By offering insights into cutting-edge developments, this primer not only equips researchers and practitioners with a solid foundation in BNNs, but also illuminates the potential applications of this dynamic field. As a valuable resource, it fosters an understanding of BNNs and their promising prospects, facilitating further advancements in the pursuit of knowledge and innovation.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Computation,Statistics - Machine Learning,Statistics - Statistics Theory},
  file = {C:\Users\ludov\Zotero\storage\CIBHUGNK\Arbel et al. - 2023 - A Primer on Bayesian Neural Networks Review and Debates.pdf}
}

@article{claessenHeteroscedasticUncertaintyQuantification2024,
  title = {Heteroscedastic Uncertainty Quantification in Physics-Informed Neural Networks},
  author = {Claessen, Olivier and Shapovalova, Yuliya and Heskes, Tom},
  date = {2024},
  abstract = {Physics-informed neural networks (PINNs) provide a machine learning framework to solve differential equations. However, PINNs do not inherently consider measurement noise or model uncertainty. In this paper, we propose the UQ-PINN which is an extension of the PINN with additional outputs to approximate noise. The multi-output architecture enables the approximation of the mean and standard deviation in data using negative Gaussian log-likelihood loss. The performance of the UQ-PINN is demonstrated on the Poisson equation with additive noise.},
  langid = {english},
  file = {C:\Users\ludov\Zotero\storage\HT24NKUK\Claessen et al. - 2024 - HETEROSCEDASTIC UNCERTAINTY QUANTIFICATION IN PHYSICS-INFORMED NEURAL NETWORKS.pdf}
}

@article{deflorioQuantificationTotalUncertainty2025,
  title = {Quantification of Total Uncertainty in the Physics-Informed Reconstruction of {{CVSim-6}} Physiology},
  author = {De Florio, Mario and Zou, Zongren and Schiavazzi, Daniele E. and Karniadakis, George Em},
  date = {2025-03-13},
  journaltitle = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {383},
  number = {2292},
  pages = {20240221},
  publisher = {Royal Society},
  doi = {10.1098/rsta.2024.0221},
  url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2024.0221},
  urldate = {2025-08-24},
  abstract = {When predicting physical phenomena through simulation, quantification of the total uncertainty due to multiple sources is as crucial as making sure the underlying numerical model is accurate. Possible sources include irreducible aleatoric uncertainty due to noise in the data, epistemic uncertainty induced by insufficient data or inadequate parameterization and model-form uncertainty related to the use of misspecified model equations. In addition, recently proposed approaches provide flexible ways to combine information from data with full or partial satisfaction of equations that typically encode physical principles. Physics-based regularization interacts in non-trivial ways with aleatoric, epistemic and model-form uncertainty and their combination, and a better understanding of this interaction is needed to improve the predictive performance of physics-informed digital twins that operate under real conditions. To better understand this interaction, with a specific focus on biological and physiological models, this study investigates the decomposition of total uncertainty in the estimation of states and parameters of a differential system simulated with MC X-TFC, a new physics-informed approach for uncertainty quantification based on random projections and Monte Carlo sampling. After an introductory comparison between approaches for physics-informed estimation, MC X-TFC is applied to a six-compartment stiff ODE system, the CVSim-6 model, developed in the context of human physiology. The system is first analysed by progressively removing data while estimating an increasing number of parameters, and subsequently by investigating total uncertainty under model-form misspecification of nonlinear resistance in the pulmonary compartment. In particular, we focus on the interaction between the formulation of the discrepancy term and quantification of model-form uncertainty, and show how additional physics can help in the estimation process. The method demonstrates robustness and efficiency in estimating unknown states and parameters, even with limited, sparse and noisy data. It also offers great flexibility in integrating data with physics for improved estimation, even in cases of model misspecification. This article is part of the theme issue ‘Uncertainty quantification for healthcare and biological systems (Part 1)’.},
  keywords = {cardiovascular physiology,physics-informed machine learning,random-projection neural networks,time series,total uncertainty quantification},
  file = {C:\Users\ludov\Zotero\storage\TSL4QRMY\De Florio et al. - 2025 - Quantification of total uncertainty in the physics-informed reconstruction of CVSim-6 physiology.pdf}
}

@online{figueres$PINNDomainDecomposition2025,
  title = {\${{PINN}} - a {{Domain Decomposition Method}} for {{Bayesian Physics-Informed Neural Networks}}},
  author = {Figueres, Júlia Vicens and Vanderhaeghen, Juliette and Bragone, Federica and Morozovska, Kateryna and Shukla, Khemraj},
  date = {2025-05-01},
  eprint = {2504.19013},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2504.19013},
  url = {http://arxiv.org/abs/2504.19013},
  urldate = {2025-09-11},
  abstract = {Physics-Informed Neural Networks (PINNs) are a novel computational approach for solving partial differential equations (PDEs) with noisy and sparse initial and boundary data. Although, efficient quantification of epistemic and aleatoric uncertainties in big multi-scale problems remains challenging. We propose \$PINN a novel method of computing global uncertainty in PDEs using a Bayesian framework, by combining local Bayesian Physics-Informed Neural Networks (BPINN) with domain decomposition. The solution continuity across subdomains is obtained by imposing the flux continuity across the interface of neighboring subdomains. To demonstrate the effectiveness of \$PINN, we conduct a series of computational experiments on PDEs in 1D and 2D spatial domains. Although we have adopted conservative PINNs (cPINNs), the method can be seamlessly extended to other domain decomposition techniques. The results infer that the proposed method recovers the global uncertainty by computing the local uncertainty exactly more efficiently as the uncertainty in each subdomain can be computed concurrently. The robustness of \$PINN is verified by adding uncorrelated random noise to the training data up to 15\% and testing for different domain sizes.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Analysis of PDEs},
  file = {C:\Users\ludov\Zotero\storage\GBSL2M5G\Figueres et al. - 2025 - $PINN - a Domain Decomposition Method for Bayesian Physics-Informed Neural Networks.pdf}
}

@online{grafErrorAwareBPINNsImproving2022,
  title = {Error-{{Aware B-PINNs}}: {{Improving Uncertainty Quantification}} in {{Bayesian Physics-Informed Neural Networks}}},
  shorttitle = {Error-{{Aware B-PINNs}}},
  author = {Graf, Olga and Flores, Pablo and Protopapas, Pavlos and Pichara, Karim},
  date = {2022-12-14},
  eprint = {2212.06965},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2212.06965},
  url = {http://arxiv.org/abs/2212.06965},
  urldate = {2025-08-24},
  abstract = {Physics-Informed Neural Networks (PINNs) are gaining popularity as a method for solving differential equations. While being more feasible in some contexts than the classical numerical techniques, PINNs still lack credibility. A remedy for that can be found in Uncertainty Quantification (UQ) which is just beginning to emerge in the context of PINNs. Assessing how well the trained PINN complies with imposed differential equation is the key to tackling uncertainty, yet there is lack of comprehensive methodology for this task. We propose a framework for UQ in Bayesian PINNs (B-PINNs) that incorporates the discrepancy between the B-PINN solution and the unknown true solution. We exploit recent results on error bounds for PINNs on linear dynamical systems and demonstrate the predictive uncertainty on a class of linear ODEs.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\ludov\\Zotero\\storage\\2BAAJQ7V\\Graf et al. - 2022 - Error-Aware B-PINNs Improving Uncertainty Quantification in Bayesian Physics-Informed Neural Networ.pdf;C\:\\Users\\ludov\\Zotero\\storage\\9AA7Z8ZD\\2212.html}
}

@online{heSurveyUncertaintyQuantification2025,
  title = {A {{Survey}} on {{Uncertainty Quantification Methods}} for {{Deep Learning}}},
  author = {He, Wenchong and Jiang, Zhe and Xiao, Tingsong and Xu, Zelin and Li, Yukun},
  date = {2025-01-20},
  eprint = {2302.13425},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2302.13425},
  url = {http://arxiv.org/abs/2302.13425},
  urldate = {2025-09-11},
  abstract = {Deep neural networks (DNNs) have achieved tremendous success in making accurate predictions for computer vision, natural language processing, as well as science and engineering domains. However, it is also well-recognized that DNNs sometimes make unexpected, incorrect, but overconfident predictions. This can cause serious consequences in high-stake applications, such as autonomous driving, medical diagnosis, and disaster response. Uncertainty quantification (UQ) aims to estimate the confidence of DNN predictions beyond prediction accuracy. In recent years, many UQ methods have been developed for DNNs. It is of great practical value to systematically categorize these UQ methods and compare their advantages and disadvantages. However, existing surveys mostly focus on categorizing UQ methodologies from a neural network architecture’s perspective or a Bayesian perspective and ignore the source of uncertainty that each methodology can incorporate, making it difficult to select an appropriate UQ method in practice. To fill the gap, this paper presents a systematic taxonomy of UQ methods for DNNs based on the types of uncertainty sources (data uncertainty versus model uncertainty). We summarize the advantages and disadvantages of methods in each category. We show how our taxonomy of UQ methodologies can potentially help guide the choice of UQ method in different machine learning problems (e.g., active learning, robustness, and reinforcement learning). We also identify current research gaps and propose several future research directions. CCS Concepts: • Computing methodologies → Uncertainty quantification; Machine learning approaches; Knowledge representation and reasoning; • Applied computing → Physical sciences and engineering; • Information systems → Data mining.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\ludov\Zotero\storage\3RS3KRCK\He et al. - 2025 - A Survey on Uncertainty Quantification Methods for Deep Learning.pdf}
}

@article{liuFlowReconstructionUncertainty2024,
  title = {Flow Reconstruction with Uncertainty Quantification from Noisy Measurements Based on {{Bayesian}} Physics-Informed Neural Networks},
  author = {Liu, Hailong and Wang, Zhi and Deng, Rui and Wang, Shipeng and Meng, Xuhui and Xu, Chao and Cai, Shengze},
  date = {2024-11-01},
  journaltitle = {Physics of Fluids},
  shortjournal = {Physics of Fluids},
  volume = {36},
  number = {11},
  pages = {117104},
  issn = {1070-6631},
  doi = {10.1063/5.0231684},
  url = {https://doi.org/10.1063/5.0231684},
  urldate = {2025-08-24},
  abstract = {Flow reconstruction based on limited measurement data, which can be considered as a state estimation problem, constitutes a fundamental task within the realm of fluid mechanics. In recent years, the physics-informed neural networks (PINNs) have been proposed to achieve flow field reconstruction by integrating the measurements with governing equations during network training. However, the performance is compromised by the presence of high-level data noise, and the uncertainty of the reconstructed flow fields remains unattainable. In this paper, we first perform a systematic study to investigate the impact of data noise on the reconstruction result of PINNs. Subsequently, we present strategies of early stopping and loss regularization, which can suppress the overfitting issue to some extent. Ensemble learning is also employed to quantify the uncertainty of the results from vanilla PINNs. In addition, we propose to use a Bayesian framework of PINNs (BPINNs) for flow field reconstruction, which incorporates the Bayesian neural network with PINNs. It is demonstrated that BPINNs are capable of reconstructing the velocity and pressure fields from sparse and noisy velocity measurements, while providing comprehensive uncertainty quantification of the flow fields simultaneously. Compared to the vanilla PINNs, BPINNs are more accurate and robust when there is a high level of data noise. We conduct experiments on two-dimensional cavity flow and the flow past a cylinder to validate the effectiveness of the proposed methods throughout the paper.},
  file = {C:\Users\ludov\Zotero\storage\PCJY88MR\5.html}
}

@article{psarosUncertaintyQuantificationScientific2023,
  title = {Uncertainty Quantification in Scientific Machine Learning: {{Methods}}, Metrics, and Comparisons},
  shorttitle = {Uncertainty Quantification in Scientific Machine Learning},
  author = {Psaros, Apostolos F. and Meng, Xuhui and Zou, Zongren and Guo, Ling and Karniadakis, George Em},
  date = {2023-03-15},
  journaltitle = {Journal of Computational Physics},
  shortjournal = {Journal of Computational Physics},
  volume = {477},
  pages = {111902},
  issn = {0021-9991},
  doi = {10.1016/j.jcp.2022.111902},
  url = {https://www.sciencedirect.com/science/article/pii/S0021999122009652},
  urldate = {2025-08-24},
  abstract = {Neural networks (NNs) are currently changing the computational paradigm on how to combine data with mathematical laws in physics and engineering in a profound way, tackling challenging inverse and ill-posed problems not solvable with traditional methods. However, quantifying errors and uncertainties in NN-based inference is more complicated than in traditional methods. This is because in addition to aleatoric uncertainty associated with noisy data, there is also uncertainty due to limited data, but also due to NN hyperparameters, overparametrization, optimization and sampling errors as well as model misspecification. Although there are some recent works on uncertainty quantification (UQ) in NNs, there is no systematic investigation of suitable methods towards quantifying the total uncertainty effectively and efficiently even for function approximation, and there is even less work on solving partial differential equations and learning operator mappings between infinite-dimensional function spaces using NNs. In this work, we present a comprehensive framework that includes uncertainty modeling, new and existing solution methods, as well as evaluation metrics and post-hoc improvement approaches. To demonstrate the applicability and reliability of our framework, we present an extensive comparative study in which various methods are tested on prototype problems, including problems with mixed input-output data, and stochastic problems in high dimensions. In the Appendix, we include a comprehensive description of all the UQ methods employed. Further, to help facilitate the deployment of UQ in Scientific Machine Learning research and practice, we present and develop in [1] an open-source Python library (github.com/Crunch-UQ4MI/neuraluq), termed NeuralUQ, that is accompanied by an educational tutorial and additional computational experiments.},
  keywords = {Bayesian framework,Neural operator learning,Physics-informed neural networks,Scientific machine learning,Stochastic partial differential equations,Uncertainty quantification},
  file = {C\:\\Users\\ludov\\Zotero\\storage\\L93JWUD6\\Psaros et al. - 2023 - Uncertainty quantification in scientific machine learning Methods, metrics, and comparisons.pdf;C\:\\Users\\ludov\\Zotero\\storage\\MBCFYQJ9\\S0021999122009652.html}
}

@article{yangBPINNsBayesianPhysicsInformed2021,
  title = {B-{{PINNs}}: {{Bayesian Physics-Informed Neural Networks}} for {{Forward}} and {{Inverse PDE Problems}} with {{Noisy Data}}},
  shorttitle = {B-{{PINNs}}},
  author = {Yang, Liu and Meng, Xuhui and Karniadakis, George Em},
  date = {2021-01},
  journaltitle = {Journal of Computational Physics},
  shortjournal = {Journal of Computational Physics},
  volume = {425},
  eprint = {2003.06097},
  eprinttype = {arXiv},
  eprintclass = {stat},
  pages = {109913},
  issn = {00219991},
  doi = {10.1016/j.jcp.2020.109913},
  url = {http://arxiv.org/abs/2003.06097},
  urldate = {2025-09-11},
  abstract = {We propose a Bayesian physics-informed neural network (B-PINN) to solve both forward and inverse nonlinear problems described by partial differential equations (PDEs) and noisy data. In this Bayesian framework, the Bayesian neural network (BNN) combined with a PINN for PDEs serves as the prior while the Hamiltonian Monte Carlo (HMC) or the variational inference (VI) could serve as an estimator of the posterior. B-PINNs make use of both physical laws and scattered noisy measurements to provide predictions and quantify the aleatoric uncertainty arising from the noisy data in the Bayesian framework. Compared with PINNs, in addition to uncertainty quantification, B-PINNs obtain more accurate predictions in scenarios with large noise due to their capability of avoiding overfitting. We conduct a systematic comparison between the two different approaches for the B-PINN posterior estimation (i.e., HMC or VI), along with dropout used for quantifying uncertainty in deep neural networks. Our experiments show that HMC is more suitable than VI for the B-PINNs posterior estimation, while dropout employed in PINNs can hardly provide accurate predictions with reasonable uncertainty. Finally, we replace the BNN in the prior with a truncated Karhunen-Lo`eve (KL) expansion combined with HMC or a deep normalizing flow (DNF) model as posterior estimators. The KL is as accurate as BNN and much faster but this framework cannot be easily extended to high-dimensional problems unlike the BNN based framework.},
  langid = {english},
  keywords = {Bayesian physics-informed neural networks,Computer Science - Machine Learning,Hamiltonian Monte Carlo,Noisy data,Nonlinear PDEs,Statistics - Machine Learning,Variational inference},
  file = {C:\Users\ludov\Zotero\storage\3FYRAQ7X\Yang et al. - 2021 - B-PINNs Bayesian Physics-Informed Neural Networks for Forward and Inverse PDE Problems with Noisy D.pdf}
}

@article{zhangQuantifyingTotalUncertainty2018,
  title = {Quantifying Total Uncertainty in Physics-Informed Neural Networks for Solving Forward and Inverse Stochastic Problems},
  author = {Zhang, Dongkun and Lu, Lu and Guo, Ling and Karniadakis, George Em},
  date = {2018-09-21},
  journaltitle = {arXiv.org},
  doi = {10.1016/j.jcp.2019.07.048},
  url = {https://arxiv.org/abs/1809.08327v1},
  urldate = {2025-08-24},
  abstract = {Physics-informed neural networks (PINNs) have recently emerged as an alternative way of solving partial differential equations (PDEs) without the need of building elaborate grids, instead, using a straightforward implementation. In particular, in addition to the deep neural network (DNN) for the solution, a second DNN is considered that represents the residual of the PDE. The residual is then combined with the mismatch in the given data of the solution in order to formulate the loss function. This framework is effective but is lacking uncertainty quantification of the solution due to the inherent randomness in the data or due to the approximation limitations of the DNN architecture. Here, we propose a new method with the objective of endowing the DNN with uncertainty quantification for both sources of uncertainty, i.e., the parametric uncertainty and the approximation uncertainty. We first account for the parametric uncertainty when the parameter in the differential equation is represented as a stochastic process. Multiple DNNs are designed to learn the modal functions of the arbitrary polynomial chaos (aPC) expansion of its solution by using stochastic data from sparse sensors. We can then make predictions from new sensor measurements very efficiently with the trained DNNs. Moreover, we employ dropout to correct the over-fitting and also to quantify the uncertainty of DNNs in approximating the modal functions. We then design an active learning strategy based on the dropout uncertainty to place new sensors in the domain to improve the predictions of DNNs. Several numerical tests are conducted for both the forward and the inverse problems to quantify the effectiveness of PINNs combined with uncertainty quantification. This NN-aPC new paradigm of physics-informed deep learning with uncertainty quantification can be readily applied to other types of stochastic PDEs in multi-dimensions.},
  langid = {english},
  keywords = {Arbitrary polynomial chaos,Dropout,Physics-informed neural networks,Stochastic differential equations,Uncertainty quantification},
  file = {C\:\\Users\\ludov\\Zotero\\storage\\6JWC7TH2\\Zhang et al. - 2019 - Quantifying total uncertainty in physics-informed neural networks for solving forward and inverse st.pdf;C\:\\Users\\ludov\\Zotero\\storage\\4H9NSRAT\\S0021999119305340.html}
}
